                      :-) GROMACS - gmx mdrun, 2019.2 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar      Christian Blau   Viacheslav Bolnykh     Kevin Boyd    
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra       Alan Gray     
  Gerrit Groenhof     Anca Hamuraru    Vincent Hindriksen  M. Eric Irrgang  
  Aleksei Iupinov   Christoph Junghans     Joe Jordan     Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul    Viveca Lindahl    Magnus Lundborg     Erik Marklund   
    Pascal Merz     Pieter Meulenhoff    Teemu Murtola       Szilard Pall   
    Sander Pronk      Roland Schulz      Michael Shirts    Alexey Shvetsov  
   Alfons Sijbers     Peter Tieleman      Jon Vincent      Teemu Virolainen 
 Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2018, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2019.2
Executable:   /home/dongdong/wyz/software/gromacs-2019.2-mod-2022/bin/gmx
Data prefix:  /home/dongdong/wyz/software/gromacs-2019.2-mod-2022
Working dir:  /home/dongdong/wyz/rid-kit/examples/test_ala_drop/iter.000000/00.enhcMD/000
Process ID:   70434
Command line:
  gmx mdrun -ntmpi 1 -nt 8 -plumed plumed.bf.dat

GROMACS version:    2019.2
Precision:          single
Memory model:       64 bit
MPI library:        thread_mpi
OpenMP support:     enabled (GMX_OPENMP_MAX_THREADS = 64)
GPU support:        CUDA
SIMD instructions:  AVX_256
FFT library:        fftw-3.3.8-sse2-avx-avx2-avx2_128-avx512
RDTSCP usage:       enabled
TNG support:        enabled
Hwloc support:      disabled
Tracing support:    disabled
C compiler:         /opt/publicsoft/gcc/5.1.0/bin/gcc GNU 5.1.0
C compiler flags:    -mavx     -O2 -DNDEBUG -funroll-all-loops -fexcess-precision=fast  
C++ compiler:       /opt/publicsoft/gcc/5.1.0/bin/g++ GNU 5.1.0
C++ compiler flags:  -mavx    -std=c++11   -O2 -DNDEBUG -funroll-all-loops -fexcess-precision=fast  
CUDA compiler:      /opt/publicsoft/cuda/10.2/bin/nvcc nvcc: NVIDIA (R) Cuda compiler driver;Copyright (c) 2005-2019 NVIDIA Corporation;Built on Wed_Oct_23_19:24:38_PDT_2019;Cuda compilation tools, release 10.2, V10.2.89
CUDA compiler flags:-gencode;arch=compute_30,code=sm_30;-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=compute_75;-use_fast_math;;; ;-mavx;-std=c++11;-O2;-DNDEBUG;-funroll-all-loops;-fexcess-precision=fast;
CUDA driver:        11.20
CUDA runtime:       10.20


Running on 1 node with total 48 cores, 96 logical cores, 1 compatible GPU
Hardware detected:
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz
    Family: 6   Model: 85   Stepping: 4
    Features: aes apic avx avx2 avx512f avx512cd avx512bw avx512vl clfsh cmov cx8 cx16 f16c fma hle htt intel lahf mmx msr nonstop_tsc pcid pclmuldq pdcm pdpe1gb popcnt pse rdrnd rdtscp rtm sse2 sse3 sse4.1 sse4.2 ssse3 tdt x2apic
    Number of AVX-512 FMA units: 2
  Hardware topology: Basic
    Sockets, cores, and logical processors:
      Socket  0: [   0  48] [   1  49] [   2  50] [   3  51] [   4  52] [   5  53] [   6  54] [   7  55] [   8  56] [   9  57] [  10  58] [  11  59] [  12  60] [  13  61] [  14  62] [  15  63] [  16  64] [  17  65] [  18  66] [  19  67] [  20  68] [  21  69] [  22  70] [  23  71]
      Socket  1: [  24  72] [  25  73] [  26  74] [  27  75] [  28  76] [  29  77] [  30  78] [  31  79] [  32  80] [  33  81] [  34  82] [  35  83] [  36  84] [  37  85] [  38  86] [  39  87] [  40  88] [  41  89] [  42  90] [  43  91] [  44  92] [  45  93] [  46  94] [  47  95]
  GPU info:
    Number of GPUs detected: 1
    #0: NVIDIA GeForce RTX 2080 Ti, compute cap.: 7.5, ECC:  no, stat: compatible

Highest SIMD level requested by all nodes in run: AVX_512
SIMD instructions selected at compile time:       AVX_256
This program was compiled for different hardware than you are running on,
which could influence performance.

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
M. J. Abraham, T. Murtola, R. Schulz, S. Páll, J. C. Smith, B. Hess, E.
Lindahl
GROMACS: High performance molecular simulations through multi-level
parallelism from laptops to supercomputers
SoftwareX 1 (2015) pp. 19-25
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
S. Páll, M. J. Abraham, C. Kutzner, B. Hess, E. Lindahl
Tackling Exascale Software Challenges in Molecular Dynamics Simulations with
GROMACS
In S. Markidis & E. Laure (Eds.), Solving Software Challenges for Exascale 8759 (2015) pp. 3-27
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
S. Pronk, S. Páll, R. Schulz, P. Larsson, P. Bjelkmar, R. Apostolov, M. R.
Shirts, J. C. Smith, P. M. Kasson, D. van der Spoel, B. Hess, and E. Lindahl
GROMACS 4.5: a high-throughput and highly parallel open source molecular
simulation toolkit
Bioinformatics 29 (2013) pp. 845-54
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
B. Hess and C. Kutzner and D. van der Spoel and E. Lindahl
GROMACS 4: Algorithms for highly efficient, load-balanced, and scalable
molecular simulation
J. Chem. Theory Comput. 4 (2008) pp. 435-447
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
D. van der Spoel, E. Lindahl, B. Hess, G. Groenhof, A. E. Mark and H. J. C.
Berendsen
GROMACS: Fast, Flexible and Free
J. Comp. Chem. 26 (2005) pp. 1701-1719
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
E. Lindahl and B. Hess and D. van der Spoel
GROMACS 3.0: A package for molecular simulation and trajectory analysis
J. Mol. Mod. 7 (2001) pp. 306-317
-------- -------- --- Thank You --- -------- --------


++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
H. J. C. Berendsen, D. van der Spoel and R. van Drunen
GROMACS: A message-passing parallel molecular dynamics implementation
Comp. Phys. Comm. 91 (1995) pp. 43-56
-------- -------- --- Thank You --- -------- --------


++++ PLEASE CITE THE DOI FOR THIS VERSION OF GROMACS ++++
https://doi.org/10.5281/zenodo.2636382
-------- -------- --- Thank You --- -------- --------

Input Parameters:
   integrator                     = md
   tinit                          = 0
   dt                             = 0.002
   nsteps                         = 25000
   init-step                      = 0
   simulation-part                = 1
   comm-mode                      = Linear
   nstcomm                        = 1000
   bd-fric                        = 0
   ld-seed                        = -314360123
   emtol                          = 10
   emstep                         = 0.01
   niter                          = 20
   fcstep                         = 0
   nstcgsteep                     = 1000
   nbfgscorr                      = 10
   rtpi                           = 0.05
   nstxout                        = 25
   nstvout                        = 25
   nstfout                        = 25
   nstlog                         = 0
   nstcalcenergy                  = 25
   nstenergy                      = 25
   nstxout-compressed             = 25
   compressed-x-precision         = 1000
   cutoff-scheme                  = Verlet
   nstlist                        = 10
   ns-type                        = Grid
   pbc                            = xyz
   periodic-molecules             = false
   verlet-buffer-tolerance        = -1
   rlist                          = 1
   coulombtype                    = PME
   coulomb-modifier               = Potential-shift
   rcoulomb-switch                = 0
   rcoulomb                       = 0.9
   epsilon-r                      = 1
   epsilon-rf                     = 80
   vdw-type                       = Cut-off
   vdw-modifier                   = Potential-shift
   rvdw-switch                    = 0
   rvdw                           = 0.9
   DispCorr                       = EnerPres
   table-extension                = 1
   fourierspacing                 = 0.12
   fourier-nx                     = 20
   fourier-ny                     = 20
   fourier-nz                     = 20
   pme-order                      = 4
   ewald-rtol                     = 1e-05
   ewald-rtol-lj                  = 0.001
   lj-pme-comb-rule               = Geometric
   ewald-geometry                 = 0
   epsilon-surface                = 0
   tcoupl                         = V-rescale
   nsttcouple                     = 10
   nh-chain-length                = 0
   print-nose-hoover-chain-variables = false
   pcoupl                         = Parrinello-Rahman
   pcoupltype                     = Isotropic
   nstpcouple                     = 10
   tau-p                          = 1.5
   compressibility (3x3):
      compressibility[    0]={ 4.50000e-05,  0.00000e+00,  0.00000e+00}
      compressibility[    1]={ 0.00000e+00,  4.50000e-05,  0.00000e+00}
      compressibility[    2]={ 0.00000e+00,  0.00000e+00,  4.50000e-05}
   ref-p (3x3):
      ref-p[    0]={ 1.00000e+00,  0.00000e+00,  0.00000e+00}
      ref-p[    1]={ 0.00000e+00,  1.00000e+00,  0.00000e+00}
      ref-p[    2]={ 0.00000e+00,  0.00000e+00,  1.00000e+00}
   refcoord-scaling               = No
   posres-com (3):
      posres-com[0]= 0.00000e+00
      posres-com[1]= 0.00000e+00
      posres-com[2]= 0.00000e+00
   posres-comB (3):
      posres-comB[0]= 0.00000e+00
      posres-comB[1]= 0.00000e+00
      posres-comB[2]= 0.00000e+00
   QMMM                           = false
   QMconstraints                  = 0
   QMMMscheme                     = 0
   MMChargeScaleFactor            = 1
qm-opts:
   ngQM                           = 0
   constraint-algorithm           = Lincs
   continuation                   = false
   Shake-SOR                      = false
   shake-tol                      = 0.0001
   lincs-order                    = 4
   lincs-iter                     = 1
   lincs-warnangle                = 30
   nwall                          = 0
   wall-type                      = 9-3
   wall-r-linpot                  = -1
   wall-atomtype[0]               = -1
   wall-atomtype[1]               = -1
   wall-density[0]                = 0
   wall-density[1]                = 0
   wall-ewald-zfac                = 3
   pull                           = false
   awh                            = false
   rotation                       = false
   interactiveMD                  = false
   disre                          = No
   disre-weighting                = Conservative
   disre-mixed                    = false
   dr-fc                          = 1000
   dr-tau                         = 0
   nstdisreout                    = 100
   orire-fc                       = 0
   orire-tau                      = 0
   nstorireout                    = 100
   free-energy                    = no
   cos-acceleration               = 0
   deform (3x3):
      deform[    0]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      deform[    1]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
      deform[    2]={ 0.00000e+00,  0.00000e+00,  0.00000e+00}
   simulated-tempering            = false
   swapcoords                     = no
   userint1                       = 0
   userint2                       = 0
   userint3                       = 0
   userint4                       = 0
   userreal1                      = 0
   userreal2                      = 0
   userreal3                      = 0
   userreal4                      = 0
   applied-forces:
     electric-field:
       x:
         E0                       = 0
         omega                    = 0
         t0                       = 0
         sigma                    = 0
       y:
         E0                       = 0
         omega                    = 0
         t0                       = 0
         sigma                    = 0
       z:
         E0                       = 0
         omega                    = 0
         t0                       = 0
         sigma                    = 0
grpopts:
   nrdf:     2295.06     44.9424
   ref-t:         300         300
   tau-t:         0.2         0.2
annealing:          No          No
annealing-npoints:           0           0
   acc:	           0           0           0
   nfreeze:           N           N           N
   energygrp-flags[  0]: 0


For optimal performance with a GPU nstlist (now 10) should be larger.
The optimum depends on your CPU and GPU resources.
You might want to try several nstlist values.
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Using 1 MPI thread
Using 8 OpenMP threads 

1 GPU selected for this run.
Mapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:
  PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged interactions on the GPU
PME tasks will do all aspects on the GPU

NOTE: The number of threads is not equal to the number of (logical) cores
      and the -pin option is set to auto: will not pin threads to cores.
      This can lead to significant performance degradation.
      Consider using -pin on (and -pinoffset in case you run multiple jobs).
System total charge: -0.000
Will do PME sum in reciprocal space for electrostatic interactions.

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
U. Essmann, L. Perera, M. L. Berkowitz, T. Darden, H. Lee and L. G. Pedersen 
A smooth particle mesh Ewald method
J. Chem. Phys. 103 (1995) pp. 8577-8592
-------- -------- --- Thank You --- -------- --------

Using a Gaussian width (1/beta) of 0.288146 nm for Ewald
Potential shift: LJ r^-12: -3.541e+00 r^-6: -1.882e+00, Ewald -1.111e-05
Initialized non-bonded Ewald correction tables, spacing: 8.85e-04 size: 1018

Long Range LJ corr.: <C6> 2.8366e-04
Generated table with 1000 data points for Ewald.
Tabscale = 500 points/nm
Generated table with 1000 data points for LJ6.
Tabscale = 500 points/nm
Generated table with 1000 data points for LJ12.
Tabscale = 500 points/nm
Generated table with 1000 data points for 1-4 COUL.
Tabscale = 500 points/nm
Generated table with 1000 data points for 1-4 LJ6.
Tabscale = 500 points/nm
Generated table with 1000 data points for 1-4 LJ12.
Tabscale = 500 points/nm

Using GPU 8x8 nonbonded short-range kernels

Using a 8x4 pair-list setup:
  updated every 10 steps, buffer 0.100 nm, rlist 1.000 nm

Using Lorentz-Berthelot Lennard-Jones combination rule

Removing pbc first time

Initializing LINear Constraint Solver

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
B. Hess and H. Bekker and H. J. C. Berendsen and J. G. E. M. Fraaije
LINCS: A Linear Constraint Solver for molecular simulations
J. Comp. Chem. 18 (1997) pp. 1463-1472
-------- -------- --- Thank You --- -------- --------

The number of constraints is 21

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
S. Miyamoto and P. A. Kollman
SETTLE: An Analytical Version of the SHAKE and RATTLE Algorithms for Rigid
Water Models
J. Comp. Chem. 13 (1992) pp. 952-962
-------- -------- --- Thank You --- -------- --------

Center of mass motion removal mode is Linear
We have the following groups for center of mass motion removal:
  0:  rest

++++ PLEASE READ AND CITE THE FOLLOWING REFERENCE ++++
G. Bussi, D. Donadio and M. Parrinello
Canonical sampling through velocity rescaling
J. Chem. Phys. 126 (2007) pp. 014101
-------- -------- --- Thank You --- -------- --------

There are: 1171 Atoms

Constraining the starting coordinates (step 0)

Constraining the coordinates at t0-dt (step 0)
RMS relative constraint deviation after constraining: 1.54e-05
Initial temperature: 307.016 K

PLUMED: PLUMED is starting
PLUMED: Version: 2.5.2 (git: Unknown) compiled on Feb 26 2022 at 14:55:59
PLUMED: Please cite this paper when using PLUMED [1]
PLUMED: For further information see the PLUMED web page at http://www.plumed.org
PLUMED: Root: /home/dongdong/wyz/software/plumed-2.5.2/
PLUMED: For installed feature, see /home/dongdong/wyz/software/plumed-2.5.2//src/config/config.txt
PLUMED: Molecular dynamics engine: gromacs
PLUMED: Precision of reals: 4
PLUMED: Running over 1 node
PLUMED: Number of threads: 8
PLUMED: Cache line size: 512
PLUMED: Number of atoms: 1171
PLUMED: File suffix: 
PLUMED: FILE: plumed.bf.dat
PLUMED: Action TORSION
PLUMED:   with label dih-001-00
PLUMED:   between atoms 5 7 9 15
PLUMED:   using periodic boundary conditions
PLUMED: Action TORSION
PLUMED:   with label dih-001-01
PLUMED:   between atoms 7 9 15 17
PLUMED:   using periodic boundary conditions
PLUMED: Action PRINT
PLUMED:   with label @2
PLUMED:   with stride 25
PLUMED:   with arguments dih-001-00 dih-001-01
PLUMED:   on file plm.out
PLUMED:   with format  %f
PLUMED: END FILE: plumed.bf.dat
PLUMED: Timestep: 0.002000
PLUMED: KbT: 2.494339
PLUMED: Relevant bibliography:
PLUMED:   [1] Tribello, Bonomi, Branduardi, Camilloni, and Bussi, Comput. Phys. Commun. 185, 604 (2014)
PLUMED: Please read and cite where appropriate!
PLUMED: Finished setup
Started mdrun on rank 0 Wed Mar 23 20:33:24 2022

           Step           Time
              0        0.00000

   Energies (kJ/mol)
          Angle    Proper Dih.  Improper Dih.          LJ-14     Coulomb-14
    3.70193e+01    4.67381e+01    2.18100e+00    9.92309e+00    1.83897e+02
        LJ (SR)  Disper. corr.   Coulomb (SR)   Coul. recip.      Potential
    2.60486e+03   -1.90839e+02   -1.82569e+04    1.47266e+02   -1.54159e+04
    Kinetic En.   Total Energy  Conserved En.    Temperature Pres. DC (bar)
    2.96726e+03   -1.24486e+04   -1.24479e+04    3.05025e+02   -2.71477e+02
 Pressure (bar)   Constr. rmsd
    4.18859e+02    3.79067e-05

step   20: timed with pme grid 20 20 20, coulomb cutoff 0.900: 6.8 M-cycles
step   20: the box size limits the PME load balancing to a coulomb cut-off of 0.900
              optimal pme grid 20 20 20, coulomb cutoff 0.900
           Step           Time
          25000       50.00000

Writing checkpoint, step 25000 at Wed Mar 23 20:33:31 2022


   Energies (kJ/mol)
          Angle    Proper Dih.  Improper Dih.          LJ-14     Coulomb-14
    2.51738e+01    4.51991e+01    7.83509e-01    1.63644e+01    2.03117e+02
        LJ (SR)  Disper. corr.   Coulomb (SR)   Coul. recip.      Potential
    2.33624e+03   -1.91291e+02   -1.80475e+04    1.40509e+02   -1.54714e+04
    Kinetic En.   Total Energy  Conserved En.    Temperature Pres. DC (bar)
    2.86301e+03   -1.26084e+04   -1.24489e+04    2.94309e+02   -2.72762e+02
 Pressure (bar)   Constr. rmsd
   -1.64666e+02    1.55984e-05

	<======  ###############  ==>
	<====  A V E R A G E S  ====>
	<==  ###############  ======>

	Statistics over 25001 steps using 1001 frames

   Energies (kJ/mol)
          Angle    Proper Dih.  Improper Dih.          LJ-14     Coulomb-14
    3.67150e+01    4.45633e+01    2.78630e+00    1.45061e+01    1.97880e+02
        LJ (SR)  Disper. corr.   Coulomb (SR)   Coul. recip.      Potential
    2.43341e+03   -1.88705e+02   -1.81729e+04    1.47244e+02   -1.54845e+04
    Kinetic En.   Total Energy  Conserved En.    Temperature Pres. DC (bar)
    2.92672e+03   -1.25578e+04   -1.24534e+04    3.00858e+02   -2.65510e+02
 Pressure (bar)   Constr. rmsd
   -5.74833e+00    0.00000e+00

          Box-X          Box-Y          Box-Z
    2.27830e+00    2.27830e+00    2.27830e+00

   Total Virial (kJ/mol)
    9.95788e+02   -3.82197e+00   -2.15185e-01
   -3.76268e+00    9.71145e+02    1.64250e+00
   -2.09565e-01    1.56886e+00    9.69785e+02

   Pressure (bar)
   -5.76004e+01    9.23984e+00   -8.06142e+00
    9.07404e+00    1.59963e+01    1.36090e+00
   -8.07695e+00    1.56740e+00    2.43592e+01

        T-Water    T-non-Water
    3.00843e+02    3.01602e+02


       P P   -   P M E   L O A D   B A L A N C I N G

 NOTE: The PP/PME load balancing was limited by the box size,
       you might not have reached a good load balance.

 PP/PME load balancing changed the cut-off and PME settings:
           particle-particle                    PME
            rcoulomb  rlist            grid      spacing   1/beta
   initial  0.900 nm  1.000 nm      20  20  20   0.113 nm  0.288 nm
   final    0.900 nm  1.000 nm      20  20  20   0.113 nm  0.288 nm
 cost-ratio           1.00             1.00
 (note that these numbers concern only part of the total PP and PME load)


	M E G A - F L O P S   A C C O U N T I N G

 NB=Group-cutoff nonbonded kernels    NxN=N-by-N cluster Verlet kernels
 RF=Reaction-Field  VdW=Van der Waals  QSTab=quadratic-spline table
 W3=SPC/TIP3p  W4=TIP4p (single or pairs)
 V&F=Potential and force  V=Potential only  F=Force only

 Computing:                               M-Number         M-Flops  % Flops
-----------------------------------------------------------------------------
 Pair Search distance check             178.733312        1608.600     0.1
 NxN Ewald Elec. + LJ [F]             20892.631296     1378913.666    93.3
 NxN Ewald Elec. + LJ [V&F]             871.586176       93259.721     6.3
 1,4 nonbonded interactions               1.025041          92.254     0.0
 Shift-X                                  2.928671          17.572     0.0
 Angles                                   0.900036         151.206     0.0
 Propers                                  0.950038         217.559     0.0
 Impropers                                0.100004          20.801     0.0
 Virial                                   3.649216          65.686     0.0
 Stop-CM                                  0.031617           0.316     0.0
 Calc-Ekin                               11.712342         316.233     0.0
 Lincs                                    0.525063          31.504     0.0
 Lincs-Mat                               10.801296          43.205     0.0
 Constraint-V                            29.777382         238.219     0.0
 Constraint-Vir                           3.511170          84.268     0.0
 Settle                                   9.576149        3093.096     0.2
-----------------------------------------------------------------------------
 Total                                                 1478153.905   100.0
-----------------------------------------------------------------------------


     R E A L   C Y C L E   A N D   T I M E   A C C O U N T I N G

On 1 MPI rank, each using 8 OpenMP threads

 Computing:          Num   Num      Call    Wall time         Giga-Cycles
                     Ranks Threads  Count      (s)         total sum    %
-----------------------------------------------------------------------------
 Neighbor search        1    8       2501       0.639         12.773   8.7
 Launch GPU ops.        1    8      50002       2.022         40.433  27.6
 Force                  1    8      25001       0.753         15.062  10.3
 Wait PME GPU gather    1    8      25001       0.037          0.742   0.5
 Reduce GPU PME F       1    8      25001       0.174          3.470   2.4
 Wait GPU NB local      1    8      25001       0.046          0.916   0.6
 NB X/F buffer ops.     1    8      47501       0.407          8.148   5.6
 Write traj.            1    8       1001       0.891         17.816  12.2
 Update                 1    8      25001       0.339          6.775   4.6
 Constraints            1    8      25003       1.010         20.209  13.8
 Rest                                           0.996         19.925  13.6
-----------------------------------------------------------------------------
 Total                                          7.313        146.269 100.0
-----------------------------------------------------------------------------

               Core t (s)   Wall t (s)        (%)
       Time:       58.317        7.313      797.4
                 (ns/day)    (hour/ns)
Performance:      590.717        0.041
Finished mdrun on rank 0 Wed Mar 23 20:33:31 2022

PLUMED:                                               Cycles        Total      Average      Minumum      Maximum
PLUMED:                                                    1     0.862657     0.862657     0.862657     0.862657
PLUMED: 1 Prepare dependencies                         25001     0.022336     0.000001     0.000001     0.000019
PLUMED: 2 Sharing data                                  1001     0.079601     0.000080     0.000044     0.016926
PLUMED: 3 Waiting for data                              1001     0.005525     0.000006     0.000004     0.000023
PLUMED: 4 Calculating (forward loop)                    1001     0.079217     0.000079     0.000062     0.000137
PLUMED: 5 Applying (backward loop)                      1001     0.035506     0.000035     0.000029     0.000056
PLUMED: 6 Update                                        1001     0.024687     0.000025     0.000016     0.000091
